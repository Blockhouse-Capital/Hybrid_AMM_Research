{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4e9623-e7d9-4a67-bb8f-03486b2884c9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6d1a1996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Complete\n"
     ]
    }
   ],
   "source": [
    "# From Imports\n",
    "from math import log, sqrt, pi, exp\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime, date\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "from scipy.optimize import minimize\n",
    "from arch import arch_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Alias Imports\n",
    "import numpy_financial as npf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Imports\n",
    "import math\n",
    "import arch\n",
    "import openpyxl \n",
    "import pprint\n",
    "\n",
    "print(\"Import Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc6470",
   "metadata": {},
   "source": [
    "# Importing Bond Trading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1c6aabec-2c22-460b-bdaa-1936f624d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel Data Imported\n"
     ]
    }
   ],
   "source": [
    "# Importing Excel data of 2Y and 10Y Tbills\n",
    "tbill2Y = pd.read_excel(\"Treasury_Data_V1.xlsx\", \"T 4.625 02 28 25 Govt TRADES\")\n",
    "tbill10Y = pd.read_excel(\"Treasury_Data_V1.xlsx\", \"T 3.375 05 15 33 Govt TRADES\")\n",
    "#note, volume is traded in increments of $1000 USD at STGT Exchange\n",
    "#note, need to clean 10Y data, bids / asks don't line up\n",
    "\n",
    "# Making Trade Time, Trade Volume, and Ask Time into lists for 2Y Bond Data\n",
    "dates2Y = list(tbill2Y[\"Trade Time\"])\n",
    "volumes2Y = list(tbill2Y[\"Trade Volume\"])\n",
    "asks2Y = list(tbill2Y[\"Ask Time\"]) # Currently Not Used\n",
    "\n",
    "print(\"Excel Data Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f38ec4-1ebf-4df6-8093-32a3d43a799d",
   "metadata": {},
   "source": [
    "# Cleaning Bond Trading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c7c224f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned\n"
     ]
    }
   ],
   "source": [
    "# Cleaning Trade Time data\n",
    "clean_dates2Y = []\n",
    "\n",
    "# Going through the range and converting dates to Numpy datetime64s\n",
    "for i in range(len(dates2Y)):\n",
    "    date = np.datetime64(str(dates2Y[i].date()))\n",
    "\n",
    "    # Removing all NaT values from the list\n",
    "    if not np.isnat(date):\n",
    "\n",
    "        # Adding all dates to our clean_dates2Y list\n",
    "        clean_dates2Y.append(date)\n",
    "\n",
    "    # If no date given, we make datetime max value for easy spotting\n",
    "    else:\n",
    "        clean_dates2Y.append(np.datetime64(datetime.max))\n",
    "\n",
    "# Cleaning Ask Time data\n",
    "clean_asks2Y = []\n",
    "\n",
    "# Going through the range and converting dates to Numpy datetime64s\n",
    "for i in range(len(asks2Y)):\n",
    "    date = np.datetime64(str(asks2Y[i].date()))\n",
    "\n",
    "    # Removing all NaT values from the list\n",
    "    if not np.isnat(date):\n",
    "\n",
    "        # Adding all dates to our clean_asks2Y list\n",
    "        clean_asks2Y.append(date)\n",
    "\n",
    "    # If no date given, we make datetime max value for easy spotting\n",
    "    else:\n",
    "        clean_dates2Y.append(np.datetime64(datetime.min))\n",
    "\n",
    "print(\"Data cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3388c-b01f-4057-8372-68af3ee72d88",
   "metadata": {},
   "source": [
    "# Calculating Average Daily Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7a413002-4525-488f-b9d9-f3984801d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new Date object column to data\n",
    "tbill2Y['Trade Time Date'] = clean_dates2Y\n",
    "tbill2Y['Ask Time Date'] = clean_asks2Y\n",
    "\n",
    "# Calculating the total volume on each specific day     \n",
    "day_volumes = tbill2Y.groupby(['Trade Time Date'])['Trade Volume'].sum()\n",
    "\n",
    "# Getting all the unique dates from Trade Time Date\n",
    "unique_dates = tbill2Y['Trade Time Date'].unique()\n",
    "\n",
    "# Creating Average Hourly Trading Volumes from Daily Volumes in Dictionary\n",
    "average_volume_daily = {} #key is date, value is total_volume\n",
    "\n",
    "# Creating dictionary around unique dates, and summed daily volumes\n",
    "for day, vol in zip(unique_dates, day_volumes):\n",
    "    \n",
    "    # Calculate the average volume for the date and store it in the dictionary\n",
    "    average_volume_daily[day] = vol / 12.0\n",
    "\n",
    "# Splitting into lists of Dates, Volumes \n",
    "dates = average_volume_daily.keys()\n",
    "hourly_volumes = average_volume_daily.values()\n",
    "\n",
    "# Print Dates, Volumes\n",
    "# print(dates)\n",
    "# print(daily_volumes)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "# pprint.pprint(average_volume_daily)\n",
    "\n",
    "# Formatting Float in Pandas\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Print the resulting list\n",
    "daily_volumes = pd.DataFrame()\n",
    "daily_volumes['Trading Day'] = dates\n",
    "daily_volumes['Hourly Volume'] = hourly_volumes\n",
    "# daily_volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7168c29",
   "metadata": {},
   "source": [
    "# Simulating Price and Spread Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6568942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Use Cox-Ingersoll-Ross Model (CIR) model to simulate bond yield process (approximation of price process)\n",
    "#2) Use GARCH model to simulate bid-ask spreads in one step ahead forecasts\n",
    "#3) Z is simulated price, ask is Zu, bid is Zl, utilize to calculate daily portfolio value and loss\n",
    "#4) Calculate compounded interest loss\n",
    "#5) Add final portfolio loss to interest loss, print resuslts of all simulations\n",
    "#OTHER NOTES#\n",
    "#determine what k value we want to keep, assume constant k, $50B issue size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb5f929-9673-47db-86b0-9281e41d9440",
   "metadata": {},
   "source": [
    "# (HELPER FUNCTION) Calculate instantaneous LP position value as a function of k, Zl (bid), Zu (ask), and Z (price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "60f46c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_t(k, Z, Zl, Zu):\n",
    "    if Zl < Z < Zu:\n",
    "        xi = k * (Z**(1/2) - (Zl)**(1/2))\n",
    "        yi = k * (Z**(-1/2) - Zu**(-1/2))\n",
    "        alpha = xi + yi * Z\n",
    "    elif Z < Zl:\n",
    "        xi = 0\n",
    "        yi = k * ((Zl)**(-1/2) - (Zu)**(-1/2))\n",
    "        alpha = xi + yi * Z\n",
    "    elif Z > Zu:\n",
    "        xi = k * ((Zu)**(1/2) - (Zl)**(1/2))\n",
    "        yi = 0\n",
    "        alpha = xi + yi * Z\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4625fbe-5794-4723-93a7-1ecfb11b5f2f",
   "metadata": {},
   "source": [
    "# Calculating Historical Midpoint between Bid and Ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "16a31446",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_bid_data = tbill2Y[\"Bid\"]\n",
    "historical_ask_data = tbill2Y[\"Ask\"]\n",
    "historical_midpoint = (historical_bid_data + historical_ask_data) / 2\n",
    "# historical_midpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce940f-734b-4067-9389-78241f116246",
   "metadata": {},
   "source": [
    "# (HELPER FUNCTION) Calculating Price to Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6b4556a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_to_yield(price, face_value, coupon_rate, coupon_frequency, current_date, maturity_date):\n",
    "    time_to_maturity_days = (maturity_date - current_date).days\n",
    "    time_to_maturity_hours = time_to_maturity_days * 24  # Convert to hours\n",
    "    \n",
    "    full_coupon_periods = int(time_to_maturity_hours / (365.0 * 24 / coupon_frequency))\n",
    "    partial_coupon_period_hours = time_to_maturity_hours % (365.0 * 24 / coupon_frequency)\n",
    "    \n",
    "    cash_flows = [(coupon_rate * face_value) / coupon_frequency] * full_coupon_periods\n",
    "    \n",
    "    # Handle the partial coupon period\n",
    "    if partial_coupon_period_hours > 0:\n",
    "        partial_coupon_payment = (coupon_rate * face_value * partial_coupon_period_hours) / (365.0 * 24)\n",
    "        cash_flows.append(partial_coupon_payment)\n",
    "    \n",
    "    cash_flows[-1] += face_value  # Add the face value at maturity\n",
    "    yield_value = np.nan\n",
    "    \n",
    "    try:\n",
    "        yield_value = npf.irr([-price] + cash_flows)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return yield_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8bf445-754e-425c-ac08-fc8c3e1732b7",
   "metadata": {},
   "source": [
    "# (HELPER FUNCTION) Estimate CIR model parameters using historical yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7f19a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05        1.05484352 -0.00940215  0.03      ]\n"
     ]
    }
   ],
   "source": [
    "def cir_likelihood(parameters, data):\n",
    "    mu, sigma, kappa, theta = parameters\n",
    "    dt = 1 / (252.0 * 12)  # Hourly data assumed\n",
    "    n = len(data)\n",
    "    log_likelihood = 0.0\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        Zl = data[i - 1]\n",
    "        Z = data[i]\n",
    "        Z_diff = Z - Zl\n",
    "        gamma = np.sqrt(kappa**2 + 2 * sigma**2)\n",
    "        \n",
    "        log_likelihood += (\n",
    "            -(n - 1) * (np.log(2 * gamma) - np.log(sigma))\n",
    "            - (kappa + gamma) * Z_diff\n",
    "            - 2 * np.log((2 * gamma * np.exp(kappa + gamma * dt)) / (2 * gamma + (kappa + gamma) * (np.exp(gamma * dt) - 1)))\n",
    "        )\n",
    "    \n",
    "    return -log_likelihood\n",
    "\n",
    "initial_parameters = [0.05, 0.1, 0.2, 0.03]  # Initial guesses for parameters\n",
    "result = minimize(cir_likelihood, initial_parameters, args=(historical_midpoint,), method='L-BFGS-B')\n",
    "mu, sigma, kappa, theta = result.x\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd77377-6433-473f-bb26-48a119656a40",
   "metadata": {},
   "source": [
    "# Simulate bond yields based on the estimated CIR parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e34333ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1  # Time to maturity, years\n",
    "n_simulations = 1000  # Num of sims\n",
    "n_periods = 252 * 12  # Num of hourly periods in a year\n",
    "coupon_rate = 0.04625  \n",
    "coupon_frequency = 2  \n",
    "\n",
    "simulated_yields = np.zeros((n_simulations, n_periods + 1)) #init array\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    dt = T / n_periods\n",
    "    current_date = tbill2Y[\"Bid Time\"][0] \n",
    "    maturity_date = current_date + pd.DateOffset(years=T)\n",
    "    bond_price = historical_midpoint.iloc[0]  # Initial bond price at first midpoint value\n",
    "    \n",
    "    for j in range(1, len(historical_midpoint)):\n",
    "        bond_yield = price_to_yield(bond_price, 1000, coupon_rate, coupon_frequency, current_date, maturity_date)\n",
    "        Z_diff = bond_yield - historical_midpoint.iloc[j - 1]\n",
    "        gamma = np.sqrt(kappa**2 + 2 * sigma**2)\n",
    "        bond_yield += kappa * (theta - bond_yield) * dt + sigma * np.sqrt(bond_yield) * np.random.normal(0, np.sqrt(dt))  # CIR process\n",
    "        bond_price = 1000 / ((1 + bond_yield / coupon_frequency) ** (coupon_frequency * (maturity_date - current_date).days / (365 * 24)))  # Convert yield to price\n",
    "        simulated_yields[i, j] = bond_yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2668eba",
   "metadata": {},
   "source": [
    "# To Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4e18eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:      1,   Func. Count:      6,   Neg. LLF: 4171621792.4244604\n",
      "Iteration:      2,   Func. Count:     17,   Neg. LLF: 46498.273960270526\n",
      "Iteration:      3,   Func. Count:     23,   Neg. LLF: -312.03886288075597\n",
      "Iteration:      4,   Func. Count:     29,   Neg. LLF: -221.8336921775031\n",
      "Iteration:      5,   Func. Count:     36,   Neg. LLF: -321.43895915756684\n",
      "Iteration:      6,   Func. Count:     42,   Neg. LLF: -398.058801328041\n",
      "Iteration:      7,   Func. Count:     47,   Neg. LLF: -398.2437815492697\n",
      "Iteration:      8,   Func. Count:     52,   Neg. LLF: -398.58002869256984\n",
      "Iteration:      9,   Func. Count:     57,   Neg. LLF: -398.7769652002499\n",
      "Iteration:     10,   Func. Count:     62,   Neg. LLF: -398.83712684366174\n",
      "Iteration:     11,   Func. Count:     67,   Neg. LLF: -398.8454840165515\n",
      "Iteration:     12,   Func. Count:     72,   Neg. LLF: 225.1111011336955\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -398.8454938053614\n",
      "            Iterations: 13\n",
      "            Function evaluations: 78\n",
      "            Gradient evaluations: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/arch/univariate/base.py:311: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
      "estimating the model parameters. The scale of y is 0.03221. Parameter\n",
      "estimation work better when this value is between 1 and 1000. The recommended\n",
      "rescaling is 10 * y.\n",
      "\n",
      "This warning can be disabled by either rescaling y before initializing the\n",
      "model or by setting rescale=False.\n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/arch/__future__/_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m forecasted_spreads \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(garch_results\u001b[38;5;241m.\u001b[39mforecast(start\u001b[38;5;241m=\u001b[39mgarch_results\u001b[38;5;241m.\u001b[39mconditional_volatility\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], horizon\u001b[38;5;241m=\u001b[39mn_forecast_periods)\u001b[38;5;241m.\u001b[39mvariance\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calc daily portfolio loss\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m alpha_0 \u001b[38;5;241m=\u001b[39m alpha_t(\u001b[43mk\u001b[49m, bond_prices[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], forecasted_spreads[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, forecasted_spreads[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m portfolio_losses \u001b[38;5;241m=\u001b[39m [alpha_0 \u001b[38;5;241m-\u001b[39m alpha_t(k, Z, Zl, Zu) \u001b[38;5;28;01mfor\u001b[39;00m Z, Zl, Zu \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(bond_prices[:, \u001b[38;5;241m1\u001b[39m:], forecasted_spreads \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, forecasted_spreads \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)]\n\u001b[1;32m     13\u001b[0m fee_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m  \u001b[38;5;66;03m# Example fixed fee rate (1%)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "# Train a GARCH model\n",
    "garch_model = arch_model(historical_midpoint, vol='Garch', p=1, q=1)\n",
    "garch_results = garch_model.fit()\n",
    "\n",
    "# Forecast spreads from the GARCH model\n",
    "n_forecast_periods = n_periods  # Assuming the same number of forecast periods as bond prices\n",
    "forecasted_spreads = np.sqrt(garch_results.forecast(start=garch_results.conditional_volatility.shape[0], horizon=n_forecast_periods).variance.values[-1, :])\n",
    "\n",
    "# Calc daily portfolio loss\n",
    "alpha_0 = alpha_t(k, bond_prices[0, 0], forecasted_spreads[0] / 2, forecasted_spreads[0] / 2)\n",
    "portfolio_losses = [alpha_0 - alpha_t(k, Z, Zl, Zu) for Z, Zl, Zu in zip(bond_prices[:, 1:], forecasted_spreads / 2, forecasted_spreads / 2)]\n",
    "\n",
    "fee_rate = 0.01  # Example fixed fee rate (1%)\n",
    "\n",
    "# Calc opportunity cost, fix to include compoundinge interest from last period\n",
    "daily_opportunity_cost = [r * (alpha_0 - alpha_t(k, Z, Zl, Zu)) for r, Z, Zl, Zu in zip(simulated_interest_rates[:, 1:], bond_prices[:, 1:], forecasted_spreads / 2, forecasted_spreads / 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87136f64-4aa0-40ef-8125-dce577572be6",
   "metadata": {},
   "source": [
    "# Train a volume model based on historical volume data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb032fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_volume_data = daily_volumes\n",
    "\n",
    "# Prepare data for training the volume model\n",
    "data = pd.DataFrame({'Bond_Price': bond_prices[:, 1:].flatten(), 'Interest_Rate': simulated_interest_rates[:, 1:].flatten()})\n",
    "X = data.values\n",
    "y = historical_volume_data.values\n",
    "\n",
    "# Train a linear regression model, replace process with more realistic simulation later\n",
    "volume_model = LinearRegression()\n",
    "volume_model.fit(X, y)\n",
    "\n",
    "# Predict volumes for the simulated data\n",
    "simulated_volumes = volume_model.predict(X).reshape(n_simulations, n_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885891c",
   "metadata": {},
   "source": [
    "# Replacing Spread And Volume Simulation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimating OU (Ornstein–Uhlenbeck) Process Paramters for Spread Simulation\n",
    "# Historical spread data\n",
    "historical_spread = [...]  # Replace with your actual data\n",
    "\n",
    "# Function to calculate the OU likelihood\n",
    "def ou_likelihood(parameters, data):\n",
    "    mean_reversion, volatility, initial_spread = parameters\n",
    "    dt = 1  # Time step (you can adjust this based on your data frequency)\n",
    "    n = len(data)\n",
    "    log_likelihood = 0.0\n",
    "    spread = initial_spread\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        spread_diff = data[i] - spread\n",
    "        log_likelihood += -0.5 * (spread_diff / volatility) ** 2\n",
    "        log_likelihood -= 0.5 * np.log(2 * np.pi * volatility ** 2 * dt)\n",
    "        spread += mean_reversion * (initial_spread - spread) * dt\n",
    "\n",
    "    return -log_likelihood\n",
    "\n",
    "# Initial parameter guesses\n",
    "initial_parameters = [0.1, 0.05, historical_spread[0]]\n",
    "\n",
    "# Minimize the negative log-likelihood to estimate OU parameters\n",
    "result = minimize(ou_likelihood, initial_parameters, args=(historical_spread,), method='L-BFGS-B')\n",
    "\n",
    "# Extract estimated parameters\n",
    "mean_reversion, volatility, initial_spread = result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84155fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulating Spreads with OU Process\n",
    "\n",
    "n_simulations = 1000  # Number of simulations\n",
    "n_periods = len(historical_spread)  # Number of periods to simulate\n",
    "\n",
    "simulated_spreads = np.zeros((n_simulations, n_periods))\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    spread = initial_spread\n",
    "    for j in range(n_periods):\n",
    "        spread += mean_reversion * (initial_spread - spread) * dt + volatility * np.sqrt(dt) * np.random.normal(0, 1)\n",
    "        simulated_spreads[i, j] = spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulating Volume using VWAP and volume and Poisson Process\n",
    "\n",
    "# Replace with volume data\n",
    "historical_hourly_volume = [10, 15, 12, 9, 14, 11, 8, 13, 10, 16, 12, 9]\n",
    "\n",
    "# Calculate the average trading rate (λ) from historical data\n",
    "average_trading_rate = np.mean(historical_hourly_volume)\n",
    "\n",
    "# Simulate trading volume using a Poisson process\n",
    "simulated_hourly_volume = np.random.poisson(average_trading_rate, len(historical_hourly_volume))\n",
    "\n",
    "# Adjust for daily variation (for example, increase trading rate during high-activity hours)\n",
    "# Can customize this adjustment based on your market's characteristics\n",
    "simulated_hourly_volume[0] *= 1.2  # Increase volume during market open\n",
    "simulated_hourly_volume[-1] *= 1.2  # Increase volume during market close\n",
    "\n",
    "# Normalize simulated volume to match historical data\n",
    "simulated_hourly_volume *= sum(historical_hourly_volume) / sum(simulated_hourly_volume)\n",
    "\n",
    "# Create a DataFrame to store the simulated volume data\n",
    "simulated_data = pd.DataFrame({'Hourly_Volume': simulated_hourly_volume})\n",
    "\n",
    "# Simulate VWAP prices based on historical VWAP data\n",
    "# Assuming a simple linear relationship between volume and price\n",
    "simulated_data['VWAP'] = np.interp(simulated_data['Hourly_Volume'],\n",
    "                                   np.cumsum(historical_hourly_volume),\n",
    "                                   historical_hourly_vwap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d71f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_profits = forecasted_spreads * simulated_volumes * fee_rate / 2  # Divide by 2 to account for bid and ask\n",
    "\n",
    "total_daily_pnl = np.array(portfolio_losses) + np.array(daily_opportunity_cost) + daily_profits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a3763-f1f8-41d2-96ff-e813834deba4",
   "metadata": {},
   "source": [
    "# Plot the LP PnL time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(total_daily_pnl, label='LP PnL')\n",
    "plt.title('Liquidity Provider (LP) PnL Over Time')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('PnL')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcff95d-f5a7-4b12-bd83-fbc46c6ebd7d",
   "metadata": {},
   "source": [
    "# Create a heatmap of LP PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ed897-6e27-4c93-b7d2-4424a9f4b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(np.array([total_daily_pnl]), annot=True, cmap='coolwarm', fmt='.2f', cbar=True)\n",
    "plt.title('Liquidity Provider (LP) PnL Heatmap')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Simulation')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
