{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4e9623-e7d9-4a67-bb8f-03486b2884c9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Imports\n",
    "from math import log, sqrt, pi, exp\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime, date\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "from scipy.optimize import minimize\n",
    "from arch import arch_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Alias Imports\n",
    "import numpy_financial as npf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Imports\n",
    "import math\n",
    "import arch\n",
    "import openpyxl \n",
    "import pprint\n",
    "\n",
    "print(\"Import Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc6470",
   "metadata": {},
   "source": [
    "# Importing Bond Trading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6aabec-2c22-460b-bdaa-1936f624d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Excel data of 2Y and 10Y Tbills\n",
    "tbill2Y = pd.read_excel(\"Treasury_Data_V1.xlsx\", \"T 4.625 02 28 25 Govt TRADES\")\n",
    "tbill10Y = pd.read_excel(\"Treasury_Data_V1.xlsx\", \"T 3.375 05 15 33 Govt TRADES\")\n",
    "#note, volume is traded in increments of $1000 USD at STGT Exchange\n",
    "#note, need to clean 10Y data, bids / asks don't line up\n",
    "\n",
    "# Making Trade Time, Trade Volume, and Ask Time into lists for 2Y Bond Data\n",
    "dates2Y = list(tbill2Y[\"Trade Time\"])\n",
    "asks2Y = list(tbill2Y[\"Ask Time\"]) # Currently Not Used\n",
    "\n",
    "print(\"Excel Data Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f38ec4-1ebf-4df6-8093-32a3d43a799d",
   "metadata": {},
   "source": [
    "# Cleaning Bond Trading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c224f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned\n"
     ]
    }
   ],
   "source": [
    "# Cleaning Trade Time data\n",
    "clean_dates2Y = []\n",
    "\n",
    "# Going through the range and converting dates to Numpy datetime64s\n",
    "for i in range(len(dates2Y)):\n",
    "    date = np.datetime64(str(dates2Y[i].date()))\n",
    "\n",
    "    # Removing all NaT values from the list\n",
    "    if not np.isnat(date):\n",
    "\n",
    "        # Adding all dates to our clean_dates2Y list\n",
    "        clean_dates2Y.append(date)\n",
    "\n",
    "    # If no date given, we make datetime max value for easy spotting\n",
    "    else:\n",
    "        clean_dates2Y.append(np.datetime64(datetime.max))\n",
    "\n",
    "# Cleaning Ask Time data\n",
    "clean_asks2Y = []\n",
    "\n",
    "# Going through the range and converting dates to Numpy datetime64s\n",
    "for i in range(len(asks2Y)):\n",
    "    date = np.datetime64(str(asks2Y[i].date()))\n",
    "\n",
    "    # Removing all NaT values from the list\n",
    "    if not np.isnat(date):\n",
    "\n",
    "        # Adding all dates to our clean_asks2Y list\n",
    "        clean_asks2Y.append(date)\n",
    "\n",
    "    # If no date given, we make datetime max value for easy spotting\n",
    "    else:\n",
    "        clean_dates2Y.append(np.datetime64(datetime.max))\n",
    "\n",
    "print(\"Data cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3388c-b01f-4057-8372-68af3ee72d88",
   "metadata": {},
   "source": [
    "# Calculating Average Daily Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a413002-4525-488f-b9d9-f3984801d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Volumes\n"
     ]
    }
   ],
   "source": [
    "# Adding new Date object column to data\n",
    "tbill2Y['Trade Time Date'] = clean_dates2Y\n",
    "tbill2Y['Ask Time Date'] = clean_asks2Y\n",
    "\n",
    "# Calculating the total volume on each specific day     \n",
    "day_volumes = tbill2Y.groupby(['Trade Time Date'])['Trade Volume'].sum()\n",
    "\n",
    "# Getting all the unique dates from Trade Time Date\n",
    "unique_dates = tbill2Y['Trade Time Date'].unique()\n",
    "\n",
    "# Getting rid of the last row\n",
    "minus_max_date = np.resize(unique_dates, unique_dates.size - 1)\n",
    "minus_max_volume = np.resize(day_volumes, day_volumes.size - 1)\n",
    "\n",
    "# Creating Average Hourly Trading Volumes from Daily Volumes in Dictionary\n",
    "average_volume_daily = {} #key is date, value is total_volume\n",
    "\n",
    "# Creating dictionary around unique dates, and summed daily volumes\n",
    "for day, volume in zip(minus_max_date, day_volumes):\n",
    "    \n",
    "    # Calculate the average volume for the date and store it in the dictionary\n",
    "    average_volume_daily[day] = volume / 12.0\n",
    "\n",
    "# Splitting into lists of Dates, Volumes \n",
    "dates = average_volume_daily.keys()\n",
    "hourly_volumes = average_volume_daily.values()\n",
    "\n",
    "# Formatting Float in Pandas\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Print the resulting list\n",
    "daily_volumes = pd.DataFrame()\n",
    "daily_volumes['Trading Day'] = dates\n",
    "daily_volumes['Hourly Volume'] = hourly_volumes\n",
    "\n",
    "print(\"Daily Volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7168c29",
   "metadata": {},
   "source": [
    "# Simulating Price and Spread Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6568942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Use Cox-Ingersoll-Ross Model (CIR) model to simulate bond yield process (approximation of price process)\n",
    "#2) Use GARCH model to simulate bid-ask spreads in one step ahead forecasts\n",
    "#3) Z is simulated price, ask is Zu, bid is Zl, utilize to calculate daily portfolio value and loss\n",
    "#4) Calculate compounded interest loss\n",
    "#5) Add final portfolio loss to interest loss, print resuslts of all simulations\n",
    "#OTHER NOTES#\n",
    "#determine what k value we want to keep, assume constant k, $50B issue size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb5f929-9673-47db-86b0-9281e41d9440",
   "metadata": {},
   "source": [
    "# Calculate instantaneous LP position value as a function of k, Zl (bid), Zu (ask), and Z (price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60f46c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_t(k, Z, Zl, Zu):\n",
    "    Z = np.where(Z < 0, np.nan, Z)\n",
    "    Zl = np.where(Zl < 0, np.nan, Zl)\n",
    "    Zu = np.where(Zu < 0, np.nan, Zu)\n",
    "\n",
    "    xi = k * (np.sqrt(Z) - np.sqrt(Zl))\n",
    "    xi = np.where(np.isnan(xi), 0, xi)\n",
    "\n",
    "    yi = k * (np.sqrt(Zu) - np.sqrt(Zl))\n",
    "    yi = np.where(np.isnan(yi), 0, yi)\n",
    "\n",
    "    return xi + yi * Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4625fbe-5794-4723-93a7-1ecfb11b5f2f",
   "metadata": {},
   "source": [
    "# Calculating Historical Midpoint between Bid and Ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a31446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Historical Midpoint and Historical Spreads\n"
     ]
    }
   ],
   "source": [
    "historical_bid_data = tbill2Y[\"Bid\"]\n",
    "historical_ask_data = tbill2Y[\"Ask\"]\n",
    "historical_midpoint = (historical_bid_data + historical_ask_data) / 2\n",
    "historical_spread = abs(historical_bid_data - historical_ask_data)\n",
    "# historical_midpoint\n",
    "\n",
    "print(\"Calculated Historical Midpoint and Historical Spreads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce940f-734b-4067-9389-78241f116246",
   "metadata": {},
   "source": [
    "# Calculating Price to Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4556a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_to_yield(price, face_value, coupon_rate, coupon_frequency, current_date, maturity_date):\n",
    "    time_to_maturity_days = (maturity_date - current_date).days\n",
    "    time_to_maturity_hours = time_to_maturity_days * 24  # Convert to hours\n",
    "    \n",
    "    full_coupon_periods = int(time_to_maturity_hours / (365.0 * 24 / coupon_frequency))\n",
    "    partial_coupon_period_hours = time_to_maturity_hours % (365.0 * 24 / coupon_frequency)\n",
    "    \n",
    "    cash_flows = [(coupon_rate * face_value) / coupon_frequency] * full_coupon_periods\n",
    "    \n",
    "    # Handle the partial coupon period\n",
    "    if partial_coupon_period_hours > 0:\n",
    "        partial_coupon_payment = (coupon_rate * face_value * partial_coupon_period_hours) / (365.0 * 24)\n",
    "        cash_flows.append(partial_coupon_payment)\n",
    "    \n",
    "    cash_flows[-1] += face_value  # Add the face value at maturity\n",
    "    yield_value = np.nan\n",
    "    \n",
    "    try:\n",
    "        yield_value = npf.irr([-price] + cash_flows)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return yield_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8bf445-754e-425c-ac08-fc8c3e1732b7",
   "metadata": {},
   "source": [
    "# Estimate CIR model parameters using historical yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f19a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05        1.05484352 -0.00940215  0.03      ]\n"
     ]
    }
   ],
   "source": [
    "def cir_likelihood(parameters, data):\n",
    "    mu, sigma, kappa, theta = parameters\n",
    "    dt = 1 / (252.0 * 12)  # Hourly data assumed\n",
    "    n = len(data)\n",
    "    log_likelihood = 0.0\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        Zl = data[i - 1]\n",
    "        Z = data[i]\n",
    "        Z_diff = Z - Zl\n",
    "        gamma = np.sqrt(kappa**2 + 2 * sigma**2)\n",
    "        \n",
    "        log_likelihood += (\n",
    "            -(n - 1) * (np.log(2 * gamma) - np.log(sigma))\n",
    "            - (kappa + gamma) * Z_diff\n",
    "            - 2 * np.log((2 * gamma * np.exp(kappa + gamma * dt)) / (2 * gamma + (kappa + gamma) * (np.exp(gamma * dt) - 1)))\n",
    "        )\n",
    "    \n",
    "    return -log_likelihood\n",
    "\n",
    "initial_parameters = [0.05, 0.1, 0.2, 0.03]  # Initial guesses for parameters\n",
    "result = minimize(cir_likelihood, initial_parameters, args=(historical_midpoint,), method='L-BFGS-B')\n",
    "mu, sigma, kappa, theta = result.x\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd77377-6433-473f-bb26-48a119656a40",
   "metadata": {},
   "source": [
    "# Simulate bond yields based on the estimated CIR parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34333ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1  # Time to maturity, years\n",
    "n_simulations = 1000  # Num of sims\n",
    "n_periods = 252 * 12 # Num of hourly periods in a year\n",
    "coupon_rate = 0.04625  \n",
    "coupon_frequency = 2  \n",
    "\n",
    "simulated_yields = np.zeros((n_simulations, n_periods)) #init array\n",
    "bond_prices = np.zeros((n_simulations, n_periods)) #init array\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    dt = T / n_periods\n",
    "    current_date = tbill2Y[\"Bid Time\"][0] \n",
    "    maturity_date = current_date + pd.DateOffset(years=T)\n",
    "    bond_price = historical_midpoint.iloc[0]  # Initial bond price at first midpoint value\n",
    "    \n",
    "    for j in range(1, n_periods):\n",
    "        bond_yield = price_to_yield(bond_price, 1000, coupon_rate, coupon_frequency, current_date, maturity_date)\n",
    "        gamma = np.sqrt(kappa**2 + 2 * sigma**2)\n",
    "        bond_yield += kappa * (theta - bond_yield) * dt + sigma * np.sqrt(bond_yield) * np.random.normal(0, np.sqrt(dt))  # CIR process\n",
    "        bond_price = 1000 / ((1 + bond_yield / coupon_frequency) ** (coupon_frequency * (maturity_date - current_date).days / (365 * 24)))  # Convert yield to price\n",
    "        bond_prices[i, j] = bond_price \n",
    "        simulated_yields[i, j] = bond_yield\n",
    "        \n",
    "print(\"Simulated Yields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b14020-a5df-431b-bd57-e94fcd88205f",
   "metadata": {},
   "source": [
    "# Simulate Risk Free Rate Yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a74444-361a-4c95-9540-e50b6786f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1  # Time to maturity, years\n",
    "n_simulations = 1000  # Num of sims\n",
    "n_periods = 252 * 12 # Num of hourly periods in a year\n",
    "coupon_rate = 0.04625  \n",
    "coupon_frequency = 2  \n",
    "\n",
    "simulated_yields_risk_free_rate = np.zeros((n_simulations, n_periods)) #init array\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    dt = T / n_periods\n",
    "    current_date = tbill2Y[\"Bid Time\"][0] \n",
    "    maturity_date = current_date + pd.DateOffset(years=T)\n",
    "    \n",
    "    for j in range(1, n_periods):\n",
    "        bond_yield = .0525\n",
    "        gamma = np.sqrt(kappa**2 + 2 * sigma**2)\n",
    "        bond_yield += kappa * (theta - bond_yield) * dt + sigma * np.sqrt(bond_yield) * np.random.normal(0, np.sqrt(dt))  # CIR process\n",
    "        simulated_yields[i, j] = bond_yield\n",
    "\n",
    "print(\"Simulated Yields Risk Free Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885891c",
   "metadata": {},
   "source": [
    "# Estimating OU (Ornstein–Uhlenbeck) Process Paramters for Spread Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical spread data is used from above calculation\n",
    "# Function to calculate the OU likelihood\n",
    "def ou_likelihood(parameters, data):\n",
    "    mean_reversion, vol, initial_spread_min = parameters\n",
    "    dt = 1  # Time step (you can adjust this based on your data frequency)\n",
    "    log_likelihood = 0.0\n",
    "    initial_spread = initial_spread_min\n",
    "    \n",
    "    for i in range(1, len(historical_spread)):\n",
    "        spread_diff = historical_spread[i] - initial_spread\n",
    "        log_likelihood += -0.5 * (spread_diff / vol) ** 2\n",
    "        log_likelihood -= 0.5 * np.log(2 * np.pi * vol ** 2 * dt)\n",
    "        initial_spread += mean_reversion * (initial_spread_min - initial_spread) * dt\n",
    "\n",
    "    return -log_likelihood\n",
    "\n",
    "# Initial parameter guesses\n",
    "intial_mean_reversion = 0.1\n",
    "intial_vol = 0.05\n",
    "intial_min_arg = historical_spread[0]\n",
    "\n",
    "# Packing them into parameter tupe\n",
    "initial_parameters = (intial_mean_reversion, intial_vol, intial_min_arg)\n",
    "\n",
    "# Minimize the negative log-likelihood to estimate OU parameters\n",
    "result = minimize(ou_likelihood, initial_parameters, args=(historical_spread,), method='L-BFGS-B')\n",
    "\n",
    "# Extract estimated parameters\n",
    "mean_reversion, volatility, initial_spread = result.x\n",
    "\n",
    "print(\"OU Likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f797d6f-e514-4522-acff-95684d509e6a",
   "metadata": {},
   "source": [
    "# Simulating Spreads with OU Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84155fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 1000  # Num of sims\n",
    "n_periods = 252 * 12 # Num of hourly periods in a year\n",
    "\n",
    "simulated_spreads = np.zeros((n_simulations, n_periods)) #init array\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    spread = initial_spread\n",
    "    \n",
    "    for j in range(1, n_periods):\n",
    "        spread += mean_reversion * (initial_spread - spread) * dt + volatility * np.sqrt(dt) * np.random.normal(0, 1)\n",
    "        simulated_spreads[i, j] = spread\n",
    "\n",
    "print(\"Spreads Simulated with OU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228c9c7-4216-429d-9db0-e0623adc1054",
   "metadata": {},
   "source": [
    "# Simulating Volume with Poisson Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with volume data\n",
    "# Calculate the average trading rate (λ) from historical data\n",
    "\n",
    "hourly_periods = 252 * 12 # Hourly Trading Periods\n",
    "\n",
    "# Calculate sums of Hourly Volume\n",
    "total_historical_sum = 0\n",
    "for hourly_volume in hourly_volumes:\n",
    "    total_historical_sum += hourly_volume\n",
    "average_historical_trading_rate = total_historical_sum / len(hourly_volumes)\n",
    "\n",
    "# Simulate trading volume using a Poisson process\n",
    "simulated_hourly_volume = np.random.poisson(average_historical_trading_rate, hourly_periods)\n",
    "\n",
    "# Calculate sums of Hourly Volume\n",
    "total_simulated_sum = 0\n",
    "for hourly_volume in simulated_hourly_volume:\n",
    "    total_simulated_sum += hourly_volume\n",
    "average_simulated_trading_rate = total_simulated_sum / len(simulated_hourly_volume)\n",
    "\n",
    "# Adjust for daily variation (for example, increase trading rate during high-activity hours)\n",
    "# Can customize this adjustment based on your market's characteristics\n",
    "simulated_hourly_volume[0] *= 1.2  # Increase volume during market open\n",
    "simulated_hourly_volume[-1] *= 1.2  # Increase volume during market close\n",
    "\n",
    "# Normalize simulated volume to match historical data\n",
    "np.multiply(simulated_hourly_volume, total_historical_sum / total_simulated_sum)\n",
    "\n",
    "# Create a DataFrame to store the simulated volume data\n",
    "simulated_data = pd.DataFrame({'Hourly_Volume': simulated_hourly_volume})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475947ec-f2f2-4359-b850-1e95f7c1fd47",
   "metadata": {},
   "source": [
    "# Calculating Opportunity Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05320b80-c81d-4394-a2d8-7dae0751a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_opportunity_cost(alpha_0, alpha_d, risk_free_rate_data, dt_values):\n",
    "    opportunity_cost = 0.0\n",
    "    \n",
    "    for alpha, r, dt in zip(alpha_d, risk_free_rate_data, dt_values):\n",
    "        opportunity_cost += (alpha_0 - alpha) * r * dt\n",
    "    \n",
    "    return opportunity_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff44cc6-3ec7-4d54-83e1-0b742046730a",
   "metadata": {},
   "source": [
    "# Calculate portfolio losses, daily profits, and total daily PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d71f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_losses = []  # Calculate portfolio losses based on simulated_spreads and simulated_vwap\n",
    "daily_profits = []  # Calculate daily profits based on simulated_spreads and simulated_vwap\n",
    "daily_opportunity_cost = []  # Calculate daily opportunity cost\n",
    "total_daily_pnl = []  # Calculate total daily PnL\n",
    "\n",
    "# Random Variables\n",
    "fee_rate = np.full(simulated_yields_risk_free_rate.shape, 0.01)\n",
    "risk_free_rate_data = simulated_yields_risk_free_rate\n",
    "dt_values = np.full(simulated_yields.shape, 1)\n",
    "\n",
    "# k = z * y^2\n",
    "y_squared = np.multiply(simulated_hourly_volume, simulated_hourly_volume)  # y^2\n",
    "z = np.random.choice(np.array(historical_midpoint), hourly_periods) # z, z is the exchange rate (x/y) current midpoint\n",
    "k = np.multiply(y_squared, z)  # z * y^2, y supply of bond, volume hourly sim\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    alpha_0 = alpha_t(k, bond_prices[0, 0], simulated_spreads[i, 0] / 2, simulated_spreads[i, 0] / 2)\n",
    "    split_simulated_spreads = simulated_spreads[0]\n",
    "    portfolio_loss_i = [alpha_0 - alpha_t(k, Z, Zl, Zu) for Z, Zl, Zu in zip(bond_prices[i, :], simulated_spreads[i, :] / 2, simulated_spreads[i, :] / 2)]\n",
    "    daily_profits_i = split_simulated_spreads * simulated_hourly_volume * fee_rate / 2  # Adjust for bid and ask\n",
    "    \n",
    "    # Calculate daily opportunity cost based on your formula\n",
    "    daily_opportunity_cost_i = [calculate_opportunity_cost(alpha_0, alpha_d, r, dt) for alpha_d, r, dt in zip(portfolio_loss_i, risk_free_rate_data, dt_values)]\n",
    "    \n",
    "    total_daily_pnl_i = np.array(portfolio_loss_i) + np.array(daily_opportunity_cost_i) + daily_profits_i\n",
    "    \n",
    "    portfolio_losses.append(portfolio_loss_i)\n",
    "    daily_profits.append(daily_profits_i)\n",
    "    daily_opportunity_cost.append(daily_opportunity_cost_i)\n",
    "    total_daily_pnl.append(total_daily_pnl_i)\n",
    "\n",
    "portfolio_losses = np.array(portfolio_losses)\n",
    "daily_profits = np.array(daily_profits)\n",
    "daily_opportunity_cost = np.array(daily_opportunity_cost)\n",
    "total_daily_pnl = np.array(total_daily_pnl)\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a3763-f1f8-41d2-96ff-e813834deba4",
   "metadata": {},
   "source": [
    "# Plot the LP PnL time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(total_daily_pnl, label='LP PnL')\n",
    "plt.title('Liquidity Provider (LP) PnL Over Time')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('PnL')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcff95d-f5a7-4b12-bd83-fbc46c6ebd7d",
   "metadata": {},
   "source": [
    "# Create a heatmap of LP PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ed897-6e27-4c93-b7d2-4424a9f4b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(np.array([total_daily_pnl]), annot=True, cmap='coolwarm', fmt='.2f', cbar=True)\n",
    "plt.title('Liquidity Provider (LP) PnL Heatmap')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Simulation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870fa3f9-8817-42b7-b23b-633f720c91d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
